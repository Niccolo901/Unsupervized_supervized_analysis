frequency = "a",
units = "log"
)
# Create a data frame for GDP/hours
gdp_divided_hours_log <- data.frame(
Date = gdp_per_capita_log$date,
GDP_Hours_Log = gdp_per_capita_log$value / hours_per_capita_log$value
)
# Convert the tfp data from annual to quarterly
tfp_zoo <- zoo(tfp_log$value, order.by = tfp_log$date)
quarterly_dates <- seq(from = as.Date("1982-01-01"), to = as.Date("2019-12-31"), by = "quarter")
tfp_quarterly_zoo <- na.approx(tfp_zoo, xout = quarterly_dates)
tfp_quarterly_df <- data.frame(
Date = index(tfp_quarterly_zoo),
TFP_Log = coredata(tfp_quarterly_zoo)
)
# Handle the missing values in real interest rate
# Convert to zoo object
interest_rate_zoo <- zoo(real_interest_rate_log$value, order.by = real_interest_rate_log$date)
# Interpolate missing values
interest_rate_zoo_interpolated <- na.approx(interest_rate_zoo)
# Create a data frame for the interpolated data
real_interest_rate_log <- data.frame(
Date = index(interest_rate_zoo_interpolated),
value = coredata(interest_rate_zoo_interpolated)
)
# Calculate the SD for each series
sd_gdp_per_capita <- sd(gdp_per_capita_log$value, na.rm = TRUE)
sd_consumption_per_capita <- sd(consumption_per_capita_log$value, na.rm = TRUE)
sd_hours_per_capita <- sd(hours_per_capita_log$value, na.rm = TRUE)
sd_gdp_divided_hours <- sd(gdp_divided_hours_log$GDP_Hours_Log, na.rm = TRUE)
sd_investment_per_capita <- sd(investment_per_capita_log$value, na.rm = TRUE)
sd_real_wage <- sd(real_wage_log$value, na.rm = TRUE)
sd_real_interest_rate <- sd(real_interest_rate_log$value, na.rm = TRUE)
sd_tfp <- sd(tfp_quarterly_df$TFP_Log, na.rm = TRUE)
# Calulate the relative standard deviation
rsd_gdp_per_capita <- sd_gdp_per_capita / sd_gdp_per_capita
rsd_consumption_per_capita <- sd_consumption_per_capita / sd_gdp_per_capita
rsd_hours_per_capita <- sd_hours_per_capita / sd_gdp_per_capita
rsd_gdp_divided_hours <- sd_gdp_divided_hours / sd_gdp_per_capita
rsd_investment_per_capita <- sd_investment_per_capita / sd_gdp_per_capita
rsd_real_wage <- sd_real_wage / sd_gdp_per_capita
rsd_real_interest_rate <- sd_real_interest_rate / sd_gdp_per_capita
rsd_tfp <- sd_tfp / sd_gdp_per_capita
# Calculate  the firs order autocorrelation
acf_gdp_per_capita <- acf(gdp_per_capita_log$value, lag.max = 1, plot = FALSE)$acf[2]
acf_consumption_per_capita <- acf(consumption_per_capita_log$value, lag.max = 1, plot = FALSE)$acf[2]
acf_hours_per_capita <- acf(hours_per_capita_log$value, lag.max = 1, plot = FALSE)$acf[2]
acf_gdp_divided_hours <- acf(gdp_divided_hours_log$GDP_Hours_Log, lag.max = 1, plot = FALSE)$acf[2]
acf_investment_per_capita <- acf(investment_per_capita_log$value, lag.max = 1, plot = FALSE)$acf[2]
acf_real_wage <- acf(real_wage_log$value, lag.max = 1, plot = FALSE)$acf[2]
acf_real_interest_rate <- acf(real_interest_rate_log$value, lag.max = 1, plot = FALSE)$acf[2]
acf_tfp <- acf(tfp_quarterly_df$TFP_Log, lag.max = 1, plot = FALSE)$acf[2]
# Calculate the correlation with GDP per capita
cor_gdp_per_capita_gdp_per_capita <- cor(gdp_per_capita_log$value, gdp_per_capita_log$value, use = "complete.obs")
cor_gdp_per_capita_consumption_per_capita <- cor(gdp_per_capita_log$value, consumption_per_capita_log$value, use = "complete.obs")
cor_gdp_per_capita_hours_per_capita <- cor(gdp_per_capita_log$value, hours_per_capita_log$value, use = "complete.obs")
cor_gdp_per_capita_gdp_divided_hours <- cor(gdp_per_capita_log$value, gdp_divided_hours_log$GDP_Hours_Log, use = "complete.obs")
cor_gdp_per_capita_investment_per_capita <- cor(gdp_per_capita_log$value, investment_per_capita_log$value, use = "complete.obs")
cor_gdp_per_capita_real_wage <- cor(gdp_per_capita_log$value, real_wage_log$value, use = "complete.obs")
cor_gdp_per_capita_real_interest_rate <- cor(gdp_per_capita_log$value, real_interest_rate_log$value, use = "complete.obs")
cor_gdp_per_capita_tfp <- cor(gdp_per_capita_log$value, tfp_quarterly_df$TFP_Log, use = "complete.obs")
# Create a data frame with the results
results <- data.frame(
Variable = c("Y", "C", "H", "Y/N", "I", "w", "r", "A"),
SD = c(sd_gdp_per_capita, sd_consumption_per_capita, sd_hours_per_capita, sd_gdp_divided_hours, sd_investment_per_capita, sd_real_wage, sd_real_interest_rate, sd_tfp),
Relative_SD = c(rsd_gdp_per_capita, rsd_consumption_per_capita, rsd_hours_per_capita, rsd_gdp_divided_hours, rsd_investment_per_capita, rsd_real_wage, rsd_real_interest_rate, rsd_tfp),
ACF = c(acf_gdp_per_capita, acf_consumption_per_capita, acf_hours_per_capita, acf_gdp_divided_hours, acf_investment_per_capita, acf_real_wage, acf_real_interest_rate, acf_tfp),
Correlation_with_GDP_per_capita = c(cor_gdp_per_capita_gdp_per_capita, cor_gdp_per_capita_consumption_per_capita, cor_gdp_per_capita_hours_per_capita, cor_gdp_per_capita_gdp_divided_hours, cor_gdp_per_capita_investment_per_capita, cor_gdp_per_capita_real_wage, cor_gdp_per_capita_real_interest_rate, cor_gdp_per_capita_tfp)
)
#Create a formatted table using kable
kable(results, format = "html", table.attr = "class='table table-striped'") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = F, position = "center") %>%
column_spec(1, bold = TRUE, color = "white", background = "orange") %>%
column_spec(2:ncol(results), border_left = TRUE, background = "#FFE4B2")
# Starting with Non Durable consumption
# Fetch Real GDP data
gdp_data <- fredr(series_id = "GDPC1",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Fetch Personal Non Durable Consumption Expenditures data
ndconsumption_data <- fredr(series_id = "PCEND",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Merge the data on the date
data_merged <- merge(gdp_data[, c("date", "value")], ndconsumption_data[, c("date", "value")],
by = "date", suffixes = c("_gdp", "_consumption"))
# Rename columns for clarity
names(data_merged) <- c("date", "gdp", "consumption")
# Compute the standard deviations with NA values ignored
sd_gdp <- sd(data_merged$gdp, na.rm = TRUE)
sd_consumption <- sd(data_merged$consumption, na.rm = TRUE)
# Print the results
cat("Standard Deviation of GDP:", sd_gdp, "\n")
cat("Standard Deviation of Consumption:", sd_consumption, "\n")
# Check if consumption is smoother than output
if (sd_consumption < sd_gdp) {
cat("Non durable consumption is smoother than output.\n")
} else {
cat("Non durable consumption is not smoother than output.\n")
}
# Moving on with Durable consumption
# Fetch Real GDP data
gdp_data <- fredr(series_id = "GDPC1",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Fetch Personal Durable Consumption Expenditures data
dconsumption_data <- fredr(series_id = "PCEDG",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Merge the data on the date
data_merged <- merge(gdp_data[, c("date", "value")], dconsumption_data[, c("date", "value")],
by = "date", suffixes = c("_gdp", "_consumption"))
# Rename columns for clarity
names(data_merged) <- c("date", "gdp", "consumption")
# Compute the standard deviations with NA values ignored
sd_gdp <- sd(data_merged$gdp, na.rm = TRUE)
sd_consumption <- sd(data_merged$consumption, na.rm = TRUE)
# Print the results
cat("Standard Deviation of GDP:", sd_gdp, "\n")
cat("Standard Deviation of Consumption:", sd_consumption, "\n")
# Check if consumption is smoother than output
if (sd_consumption < sd_gdp) {
cat("Durable consumption is smoother than output.\n")
} else {
cat("Durable consumption is not smoother than output.\n")
}
# Analyze Non-Durable Consumption
# Retrieve Real GDP data in log form
gdp_data <- fredr(series_id = "GDPC1",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "log")
# Retrieve Non-Durable Consumption Expenditures data in log form
ndconsumption_data <- fredr(series_id = "PCEND",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "log")
# Combine data based on date
data_merged <- merge(gdp_data[, c("date", "value")], ndconsumption_data[, c("date", "value")],
by = "date", suffixes = c("_gdp", "_consumption"))
# Rename columns for readability
names(data_merged) <- c("date", "gdp", "consumption")
# Calculate standard deviations, excluding NA values
sd_gdp <- sd(data_merged$gdp, na.rm = TRUE)
sd_consumption <- sd(data_merged$consumption, na.rm = TRUE)
# Display the results
cat("Standard Deviation of Log GDP:", sd_gdp, "\n")
cat("Standard Deviation of Log Consumption:", sd_consumption, "\n")
# Determine if consumption is smoother than output
if (sd_consumption < sd_gdp) {
cat("Log-transformed non-durable consumption is smoother than output.\n")
} else {
cat("Log-transformed non-durable consumption is not smoother than output.\n")
}
# Fetch Real GDP data
gdp_data <- fredr(series_id = "GDPC1",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Fetch Total Hours Worked data
hours_worked_data <- fredr(series_id = "HOANBS",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Merge the data on the date
data_merged <- merge(gdp_data[, c("date", "value")], hours_worked_data[, c("date", "value")],
by = "date", suffixes = c("_gdp", "_hours_worked"))
# Rename columns for clarity
names(data_merged) <- c("date", "gdp", "hours_worked")
# Compute the standard deviations
sd_gdp <- sd(data_merged$gdp)
sd_hours_worked <- sd(data_merged$hours_worked)
# Print the results
cat("Standard Deviation of GDP:", sd_gdp, "\n")
cat("Standard Deviation of Total Hours Worked:", sd_hours_worked, "\n")
# Check if the volatilities are similar
if (abs(sd_gdp - sd_hours_worked) / sd_gdp < 0.1) {
cat("Volatility in GDP is similar in magnitude to volatility in total hours worked.\n")
} else {
cat("Volatility in GDP is not similar in magnitude to volatility in total hours worked.\n")
}
# Fetch Total Nonfarm Payrolls (employment) data
employment_data <- fredr(series_id = "PAYEMS",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Fetch Average Weekly Hours data
hours_data <- fredr(series_id = "AWHMAN",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Merge the data on the date
data_merged <- merge(employment_data[, c("date", "value")], hours_data[, c("date", "value")],
by = "date", suffixes = c("_employment", "_hours"))
# Rename columns for clarity
names(data_merged) <- c("date", "employment", "hours")
# Compute the standard deviations only for the available period
sd_employment <- sd(data_merged$employment, na.rm = TRUE)
sd_hours <- sd(data_merged$hours, na.rm = TRUE)
# Print the results
cat("Standard Deviation of Employment:", sd_employment, "\n")
cat("Standard Deviation of Average Hours Worked:", sd_hours, "\n")
# Check if the volatility in employment is greater than the volatility in average hours
if (sd_employment > sd_hours) {
cat("Volatility in employment is greater than volatility in average hours worked.\n")
cat("Most labor market adjustments operate on the extensive margin.\n")
} else {
cat("Volatility in employment is not greater than volatility in average hours worked.\n")
cat("Labor market adjustments may not predominantly operate on the extensive margin.\n")
}
# Fetch data for productivity and real GDP (GDPC1)
productivity_data <- fredr(series_id = "OPHNFB",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
gdp_data <- fredr(series_id = "GDPC1",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Merge the data on the date
data_merged <- merge(productivity_data[, c("date", "value")], gdp_data[, c("date", "value")],
by = "date", suffixes = c("_productivity", "_gdp"))
# Compute correlations
correlation <- cor(data_merged$value_productivity, data_merged$value_gdp)
# Interpret the correlation result
if (correlation > 0) {
cat("Productivity is pro-cyclical with GDP.\n")
} else if (correlation < 0) {
cat("Productivity is counter-cyclical with GDP.\n")
} else {
cat("There is no significant correlation between productivity and GDP.\n")
}
# Fetch data for average hourly wage and productivity
wages_data <- fredr(series_id = "CES0500000003",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
productivity_data <- fredr(series_id = "OPHNFB",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Merge the data on the date
data_merged <- merge(wages_data[, c("date", "value")], productivity_data[, c("date", "value")],
by = "date", suffixes = c("_wages", "_productivity"))
# Compute the standard deviations
sd_wages <- sd(data_merged$value_wages, na.rm = TRUE)
sd_productivity <- sd(data_merged$value_productivity, na.rm = TRUE)
# Print the results
cat("Standard Deviation of wages:", sd_wages, "\n")
cat("Standard Deviation of productivity:", sd_productivity, "\n")
# Interpret the results
if (sd_wages < sd_productivity) {
cat("Wages are less variable than productivity.\n")
} else if (sd_wages > sd_productivity) {
cat("Wages are more variable than productivity.\n")
} else {
cat("Wages and productivity have similar variability.\n")
}
# Fetch data for average hourly wage (AHE), real GDP (GDPC1), and total nonfarm payrolls (PAYEMS)
wages_data <- fredr(series_id = "CES0500000003",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
gdp_data <- fredr(series_id = "GDPC1",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
employment_data <- fredr(series_id = "PAYEMS",
observation_start = as.Date("1950-01-01"),
observation_end = as.Date("2023-01-01"),
frequency = "q",
units = "lin")
# Merge the data on the date
data_merged <- merge(wages_data[, c("date", "value")], gdp_data[, c("date", "value")], by = "date", suffixes = c("_wages", "_gdp"))
data_merged <- merge(data_merged, employment_data[, c("date", "value")], by = "date")
names(data_merged) <- c("date", "wages", "gdp", "employment")
# Compute the correlation coefficients, ignoring NA values
correlation_wages_output <- cor(data_merged$wages, data_merged$gdp, use = "complete.obs")
correlation_wages_employment <- cor(data_merged$wages, data_merged$employment, use = "complete.obs")
# Print the correlation results
cat("Correlation between wages and GDP:", correlation_wages_output, "\n")
cat("Correlation between wages and employment:", correlation_wages_employment, "\n")
# Interpret the results
if (!is.na(correlation_wages_output) && !is.na(correlation_wages_employment) &&
abs(correlation_wages_output) < 0.1 && abs(correlation_wages_employment) < 0.1) {
cat("There is no significant correlation between wages and output (GDP) or employment.\n")
} else {
cat("There is a significant correlation between wages and either output (GDP) or employment.\n")
}
# Check the predicted probabilities
predicted_probabilities <- predict(best_model_logit, newdata = test_data, type = "response")
# Load necessary libraries
library(caret)    # For model training and evaluation
library(factoextra)    # For PCA visualization
library(ROCR)     # For ROC and AUC
library(glmnet)   # For LASSO/Ridge regression
library(randomForest)  # For Random Forest
library(cluster)  # For silhouette score
library(plotly)   # For 3D plotting
library(car)      # For VIF calculation
library(xgboost)  # For XGBoost
library(pROC)  # For ROC curve visualization
library(reshape2)  # For data manipulation
library(FactoMineR)  # For PCA
library(cluster)     # For silhouette score
library(NbClust)     # For NbClust
library(clustMixType)    # For clustering mixed data types
library(kohonen)    # For SOM
library(fpc)    # For DBSCAN
library(dbscan)    # For DBSCAN
#set working directory
setwd("C:/Users/cibei/OneDrive/Desktop/Statistical and machine learning/SL_project")
# Read the preprocessed dataset from the CSV file
credit_data_final <- read.csv("credit_data_preprocessed.csv", header = TRUE)
set.seed(123)
trainIndex <- createDataPartition(credit_data_final$Approved, p = 0.8, list = FALSE)
train_data <- credit_data_final[trainIndex, ]
test_data <- credit_data_final[-trainIndex, ]
#Define a function that performs K-Fold cross-validation for logistic regression
logistic_regression_cv <- function(data, formula, k = 10) {
set.seed(123) # Set seed for reproducibility
# Set up trainControl for k-fold cross-validation
train_control <- trainControl(method = "cv",
number = k,
classProbs = TRUE,
summaryFunction = twoClassSummary,
search = "grid",
savePredictions = "final")
# Train the logistic regression model using k-fold cross-validation
logit_model_cv <- train(formula,
data = data,
method = "glm",
trControl = train_control,
metric = "ROC",
family = "binomial")
return(logit_model_cv)
}
#Perform 10-fold cross-validation for logistic regression
cv_model_logit <- logistic_regression_cv(train_data, Approved ~ .)
#Print the cross-validated results
print(cv_model_logit)
#Extract and display the cross-validation results
cv_results <- cv_model_logit$results
print(cv_results)
#Extract the best model from the cross-validated results
best_model_logit <- cv_model_logit$finalModel
print(best_model_logit)
#Summarize the best model
summary(best_model_logit)
test_data$Approved <- as.factor(test_data$Approved)
# Check the predicted probabilities
predicted_probabilities <- predict(best_model_logit, newdata = test_data, type = "response")
# Ensure predictions are created properly based on the threshold (0.5 in this case)
predictions <- ifelse(predicted_probabilities > 0.5, "Approved", "Rejected")
# Convert to a factor with levels "Approved" and "Rejected"
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
confusionMatrix(predictions, test_data$Approved)
# Ensure predictions are created properly based on the threshold (0.5 in this case)
predictions <- ifelse(predicted_probabilities > 0.8, "Approved", "Rejected")
# Convert to a factor with levels "Approved" and "Rejected"
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
confusionMatrix(predictions, test_data$Approved)
# Ensure predictions are created properly based on the threshold (0.5 in this case)
predictions <- ifelse(predicted_probabilities > 0.0, "Approved", "Rejected")
# Convert to a factor with levels "Approved" and "Rejected"
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
confusionMatrix(predictions, test_data$Approved)
# Ensure predictions are created properly based on the threshold (0.5 in this case)
predictions <- ifelse(predicted_probabilities > 0.2, "Approved", "Rejected")
# Convert to a factor with levels "Approved" and "Rejected"
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
confusionMatrix(predictions, test_data$Approved)
# Check levels in train_data and test_data
levels(train_data$Approved)
levels(test_data$Approved)
levels(predictions))
levels(predictions)
levels(test_data$Approved)
# Check the predicted probabilities
predicted_probabilities <- predict(best_model_logit, newdata = test_data, type = "response")
# Ensure predictions are created properly based on the threshold (0.5 in this case)
predictions <- ifelse(predicted_probabilities > 0.2, "Approved", "Rejected")
# Convert to a factor with levels "Approved" and "Rejected"
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
confusionMatrix(predictions, test_data$Approved)
# Ensure predictions are created properly based on the threshold (0.5 in this case)
predictions <- ifelse(predicted_probabilities > 0.4, "Approved", "Rejected")
# Convert to a factor with levels "Approved" and "Rejected"
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
confusionMatrix(predictions, test_data$Approved)
# Ensure predictions are created properly based on the threshold (0.5 in this case)
predictions <- ifelse(predicted_probabilities > 0.7, "Approved", "Rejected")
# Convert to a factor with levels "Approved" and "Rejected"
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
confusionMatrix(predictions, test_data$Approved)
confusionMatrix(predictions, test_data$Approved, positive = "Approved")
#Summarize the best model
summary(best_model_logit)
# Define a new formula with only the significant variables
significant_formula <- Approved ~ PriorDefault + CreditScore + Income +
Industry.ConsumerDiscretionary + Industry.ConsumerStaples + Industry.ConsumerStaples +
Industry.Real.Estate + Industry.Research +  Ethnicity.Latino + Ethnicity.Other
# Use the existing logistic_regression_cv function with the new formula
cv_model_logit_significant <- logistic_regression_cv(credit_data_final, significant_formula)
# Print the cross-validated results
print(cv_model_logit_significant)
# Use the existing logistic_regression_cv function with the new formula
cv_model_logit_significant <- logistic_regression_cv(train_data, significant_formula)
# Print the cross-validated results
print(cv_model_logit_significant)
# Extract and display the cross-validation results
cv_results_significant <- cv_model_logit_significant$results
print(cv_results_significant)
# Extract the best model from the cross-validated results
best_model_logit_significant <- cv_model_logit_significant$finalModel
print(summary(best_model_logit_significant))
head(predicted_probabilities)
# Predict on test data
predicted_probabilities <- predict(best_model_logit_significant, newdata = test_data, type = "response")
# Ensure the labels match the actual factor levels in the data
predictions <- ifelse(predicted_probabilities > 0.5, "Approved", "Rejected")
# Convert predictions to a factor with the same levels as test_data$Approved
predictions <- factor(predictions, levels = levels(test_data$Approved))
# Convert test_data$Approved to a factor with the correct levels
test_data$Approved <- factor(test_data$Approved, levels = c("Approved", "Rejected"))
# Convert predictions to factor with the same levels as test_data$Approved
predictions <- factor(predictions, levels = c("Approved", "Rejected"))
# Evaluate model using confusion matrix
confusionMatrix(predictions, test_data$Approved)
# Create a copy of the dataset for glmnet analysis
credit_data_glmnet <- credit_data_final
# Convert categorical variables into dummy variables (one-hot encoding)
x_data <- model.matrix(Approved ~ ., data = credit_data_glmnet)[, -1]  # Exclude intercept column
y_data <- as.numeric(credit_data_glmnet$Approved) - 1  # Convert factor Approved to numeric 0/1
# Split the dataset into training and testing sets (70% train, 30% test)
set.seed(123)
train_index <- createDataPartition(y_data, p = 0.7, list = FALSE)
x_train <- x_data[train_index, ]
x_test <- x_data[-train_index, ]
y_train <- y_data[train_index]
y_test <- y_data[-train_index]
# Fit glmnet model with L1/L2 regularization using cross-validation (Elastic Net with alpha = 0.5)
set.seed(123)
cv_glmnet_model <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0.5, nfolds = 10)  # alpha=0.5 for Elastic Net
# Print the best lambda
best_lambda <- cv_glmnet_model$lambda.min
cat("Best lambda (regularization strength):", best_lambda, "\n")
# Get the coefficients of the model
coefficients_glmnet <- coef(cv_glmnet_model, s = best_lambda)
print(coefficients_glmnet)
# Convert coefficients to a data frame and remove zero-coefficient variables
coefficients_df <- as.data.frame(as.matrix(coefficients_glmnet))
coefficients_df$Variable <- rownames(coefficients_df)
# Identify the variables with non-zero coefficients
non_zero_vars <- coefficients_df[coefficients_df$s1 != 0, "Variable"]
cat("Non-zero coefficient variables:", non_zero_vars, "\n")
# Remove the intercept from non-zero variables
non_zero_vars <- non_zero_vars[non_zero_vars != "(Intercept)"]
# Filter the training and test datasets to include only non-zero coefficient variables
x_train <- model.matrix(Approved ~ ., data = credit_data_glmnet)[train_index, non_zero_vars]
x_test <- model.matrix(Approved ~ ., data = credit_data_glmnet)[-train_index, non_zero_vars]
# Refit the logistic regression model on the updated training dataset using glmnet
cv_model_logit_final <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 0.5)
# Predict on the training data for all lambdas
train_predictions_all <- predict(cv_model_logit_final, newx = x_train, s = cv_model_logit_final$lambda, type = "response")
# Assess performance across all lambdas
performance_all_lambdas <- assess.glmnet(train_predictions_all, newy = y_train, family = "binomial")
# Create a copy of the dataset for glmnet analysis
credit_data_glmnet <- credit_data_final
# Convert categorical variables into dummy variables (one-hot encoding)
x_data <- model.matrix(Approved ~ ., data = credit_data_glmnet)[, -1]  # Exclude intercept column
y_data <- as.numeric(credit_data_glmnet$Approved) - 1  # Convert factor Approved to numeric 0/1
# Split the dataset into training and testing sets (70% train, 30% test)
set.seed(123)
train_index <- createDataPartition(y_data, p = 0.7, list = FALSE)
View(credit_data_glmnet)
